# LSTM and GRU Essentials
### An intuition of RNN with simple cooking analogy 
If we are cooking a dish, we take a single pan (function) to which we add different ingredients (inputs) one by one in a sequence but the pan remains same. During this everytime a new ingredient is added flavour(output) gets changed. Back propagation is like after finishing the dish we try to taste the sample if we feel that dish does not have enough salt (w, w') we add a little salt and check if the dish is perfect now if not we will add it again till the dish tastes perfect.

### Major difference of RNN with ML models and DL models
Generally, ML models, MLP, image classification using CNN etc, does the prediction by accepting fixed set of inputs or of fixed size.

In case of RNN, the feedback loop makes it possible to use sequential input values, like stock market price, weather data etc collected over time, to make predictions

### Problem with RNN and significance of LSTM/GRU
Partial derivative of loss function with respect to first weight w involves lots of multiplications of previous partial derivatives. This is leads to vanishing gradient problem. (The gradient is the value used to update the neural netwrok’s weight. VG problem is when a gradient shrinks as it back propagates through time. If a gradient value becomes extremely small it doesn’t contribute too much in learning). This is not because we have several layers, but we are performing BP and FP over time. 

Imagine we have a sentence of 100 words, then we will do 100 multiplications of partial derivatives. If our activation units are sigmoid/tanh then all the derivatives would be <1. Then VG problem kicks in very fast. Also, we can face exploding gradient problem, if few derivatives are >1. When RNN layers wont learn due to VG problem, RNNs will forget what is seen in longer sequences does have a short term memory. 
Simple RNNs cannot take care of long term dependency.

We will resolve this issue using LSTMs and GRUs. Both are evolved versions of RNN which will overcome short term memory. It has internal mechanisms called gates that can regulate the flow of information. These gates can learn which data in a sequence is important to keep or throw away.

### What exactly is 'unit' in LSTM? For example, we can use keras.layers.LSTM(32) with 32 is the "units". Is it the number of LSTM cells, or the number of neurons in some part of LSTM ?
-	The number of units defines the dimension of hidden states (or outputs) and the number of params in the LSTM layer. More units (greater dimension of hidden states) will help the network to remember more complex patterns.
-	For example, in the image above, the hidden state (the red circles) has length 2. The number of units is the number of neurons connected to the layer holding the concatenated vector of hidden state and input (the layer holding both red and green circles below). In this example, there are 2 neurons connected to that layer.
-	The number of units in each layer of RNN is a hyperparameter that we need to tune. They're not dependent on the number of words in a sentence.
-	Number of words in sentence == Number of time a cell will be unfolded along time axis == Maximum length of sentence among all sentences != number of units

### Explain the input and output shapes of LSTM system
**Input shape :** [no of sentences in batch, no of words in sentence, dimension in which a word is embedded]
Consider IMDB dataset of reviews for sentiment analysis which contains several reviews. Before feeding to LSTM, we will encode the input sequence. First we will set the length of per review a fixed length say for example 80 ie 80 words per review. If review length is more, we trim it or if its less, we will pad with zeroes. Then each word will be encoded using one hot encoding or w2v etc in a predefined vector length. Eg 100 ie each word will be represented using 100 length vector. 
LSTM accepts 3 inputs. First is batch size ie no of reviews we sent in one batch to process and then update the weights. Let the btach size =3 ie 3 reviews will be inputted in one batch. Second input shape is timestep, which is nothing but 80 here. Last shape is input dimension which is 100 here. Ie input shape is      [3, 80, 100] where, each review is set to 80 words and each word is of 100D vector ie each review is a 80 X 100 matrix.

**Output shape :** While defining LSTM we mentioned on units. Say for example 64 units. Means LSTM will convert 80 X 100 matrix to 64 D vector. So the output shape is [3, 64], ie each review in a batch is converted to a 64 dim vector. 

### Psuedo code for LSTM
-	Concatanate previous hidden state and current input
-	Combine will fed to forget layer. This layer removes non relevant data
-	Combine will fed to input layer which decides what data should keep (tan fn is to regulate the network as explained above)
-	Cell state is computed using the vectors generated by forget layer, input layer and candidate layer
-	Feed this to o/p layer. Pointwise multiplication will give new hidden state.

### LSTM Vs GRU
GRU use less training parameters and therefore use less memory, execute faster and train faster than LSTM's whereas LSTM is more accurate on dataset using longer sequence. In short, if sequence is large or accuracy is very critical, please go for LSTM whereas for less memory consumption and faster operation go for GRU. If you donot have much floating point operations per second (FLOP's) to spare switch to GRU. LSTM has three values at output (output, hidden and cell) whereas GRU has two values at output (output and hidden).

### Input of LSTM is supposed to be variable length time series data. Then why to apply padding on the input vector to make all data points same length?
Assume there are 3 input vectors. X1 contains 189 words, X2- 310, X3 -150. We want to back propagate each of the input, back propagate over time. What we are doing is an SGD operation with batch size =1, ie at any point of time we are processing one sequence, not combining sequences for processing. And this approach is too slow and takes lot of time. So the remedy is perform SGD with batch size = k. ie LSTM has several inputs. One input receives X11,X21,X31… (first element of every data points. So instead of processing SGD for whole inputs, the method is collecting all the first words from input vectors of batch size k as a set and sent. Similarly X12,X22,X32.. will sent simultaneously. So in order to perform based on batch we need input vector size to be same. This will speed up the training in LSTM.

### Find no of parameters in a Stacked Bidirectional GRU with 5 hidden units and input size 8 (whose outputs are concatenated) + LSTM with 50 hidden units

Bidirectional GRU with 5 hidden units and input size 8
-	g = 3 (GRU has 3 FFNNs)
-	h = 5
-	i = 8
num_params_layer1 = 2 × g × [h(h+i) + h] (first term is 2 because of bidirectionality)
         = 2 × 3 × [5(5+8) + 5] = 420
LSTM with 50 hidden units
-	g = 4 (LSTM has 4 FFNNs)
-	h = 50
-	i = 5+5 (outputs from bidirectional GRU concatenated; output size of GRU is 5, same as no. of hidden units)
num_params_layer2 = g × [h(h+i) + h]
                     = 4 × [50(50+10) + 50] = 12,200
total_params = 420 + 12,200 = 12,620
input = Input((None, 8))
layer1 = Bidirectional(GRU(5, return_sequences=True))(input)
layer2 = LSTM(50)(layer1)
model = Model(input, layer2)
